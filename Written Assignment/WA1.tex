%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[addpoints,12pt,solution]{exam}
\printanswers
\usepackage{amsmath,amssymb}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{paralist}
\usepackage{psfrag}
\usepackage{fullpage}
\usepackage{fancybox}
\usepackage{ifthen}
\usepackage{hyperref}
\usepackage{marvosym}
\usepackage{amsmath}
\usepackage[ruled,vlined,noresetcount]{algorithm2e}
\usepackage{enumitem}
\marksnotpoints


\begin{document}


\hrule
\vspace{1mm}
\noindent 
\begin{center}
{\Large CS6700 : Reinforcement Learning} \\
{\large Written Assignment \#1}
\end{center}
\vspace{1mm}
\noindent 
{\large Intro to RL, Bandits, DP   \hfill Deadline: 23 Feb 2020, 11:55 pm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Enter name and roll number here
\noindent {\bf Name:} Enter Name \hfill {\bf Roll number:} Enter Roll Number
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{2mm}
\hrule

{\small

\begin{itemize}\itemsep0mm
\item This is an individual assignment. Collaborations and discussions are strictly
prohibited.
\item Be precise with your explanations. Unnecessary verbosity will be penalized.
\item Check the Moodle discussion forums regularly for updates regarding the assignment.
\item Type your solutions in the provided \LaTeX template file.
\item \textbf{Please start early.}
\end{itemize}
}

\hrule

\vspace{3mm}




%\gradetable[h][questions]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%START HERE%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{questions}
\question[2] You have come across Median Elimination as an algorithm to get $(\epsilon, \delta)-$PAC bounds on the best arm in a bandit problem. At every round, half
of the arms are removed by removing arms with return estimates below
the median of all estimates. How would this work if we removed only
one-fourth of the worst estimated arms instead? Attempt a derivation of
the new sample complexity.

\begin{solution}
%Write solution here
\end{solution}

\question[3] Consider a bandit problem in which you know the set of expected payoffs for pulling various arms, but you do not know which arm maps to which expected payoff. For example, consider a 5 arm bandit problem and you know that the arms 1 through 5 have payoffs 3.1, 2.3, 4.6, 1.2, 0.9, but not necessarily in that order. Can you design a regret minimizing algorithm that will achieve better bounds than UCB? What makes you believe that it is possible? What parts of the analysis of UCB will you modify to achieve better bounds?
\begin{solution}
%Write solution here
\end{solution}

\question[3]Suppose you face a 2-armed bandit task whose true action values change randomly from time step to time step. Specifically, suppose that, for any time step, the true values of actions 1 and 2 are respectively 0.1 and 0.2 with probability 0.5 (case A), and 0.9 and 0.8 with probability 0.5 (case B). 

\begin{enumerate}[label=(\alph*)]
    \item (1 mark) If you are not able to tell which case you face at any step, what is the best expectation of success you can achieve and how should you behave to achieve it?
\begin{solution}
%Write solution here
\end{solution}
    
    \item (2 marks) Now suppose that on each step you are told whether you are facing case A or case B (although you still don’t know the true action values). This is an associative search task. What is the best expectation of success you can achieve in this task, and how should you behave to achieve it?
\begin{solution}
%Write solution here
\end{solution}
\end{enumerate}
 




\question[5]Many tic-tac-toe positions appear different but are really the same because of symmetries. 
\begin{enumerate}[label=(\alph*)]
    \item (2 marks) How might we amend the learning process described above to take advantage of this? In what ways would this change improve the learning process? 
    
\begin{solution}
%Write solution here
\end{solution}

    \item (1 mark) Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value? 
    
\begin{solution}
%Write solution here
\end{solution}

    \item (2 marks) Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think  would happen in this case? Would it learn a different policy for selecting moves? 
    
\begin{solution}
%Write solution here
\end{solution}

\end{enumerate}

\question[1]Ego-centric representations are based on an agent's current
position in the world. In a sense the agent says, I don’t care where
I am, but I am only worried about the position of the objects in the
world relative to me. You could think of the agent as being at the origin
always. Comment on the suitability (advantages and disadvantages)
of using an ego-centric representation in RL.

\begin{solution}
%Write solution here
\end{solution}

\question[2] Consider a general MDP with a discount factor of $\gamma$.
For this case assume that the horizon is infinite. Let $\pi$ be a policy and $V^{\pi}$ be the corresponding value function. Now suppose we have a new
MDP where the only difference is that all rewards have a constant $k$ added to them. Derive the new value function $V^{\pi}_{new}$ in terms of $V^{\pi},\ c$ and $\gamma$.

\begin{solution}
%Write solution here
\end{solution}


\question[4] An $\epsilon$-soft policy for a MDP with state set $\mathcal{S}$ and action set $\mathcal{A}$ is any policy that satisfies \[\forall a\in \mathcal{A}, \forall s \in \mathcal{S}: \pi(a|s)\geq \frac{\epsilon}{|\mathcal{A}|}\]
Design a stochastic gridworld where a deterministic policy will produce
the same trajectories as a $\epsilon$-soft policy in a deterministic
gridworld. In other words, for every trajectory under the same policy, the
probability of seeing it in each of the worlds is the same. By the same policy I mean
that in the stochastic gridworld, you have a deterministic policy and in the
deterministic gridworld, you use the same policy, except for $\epsilon$ fraction of
the actions, which you choose uniformly randomly. 
\begin{enumerate}[label=(\alph*)]
\item (2 marks) Give the complete specification of the world.


\begin{solution}
%Write solution here
\end{solution}


\item (2 marks) Will SARSA on the two worlds converge to the same policy? Justify.

\begin{solution}
%Write solution here
\end{solution}

\end{enumerate}

\question[7]You receive the following letter:\\
Dear Friend, Some time ago, I bought this old house, but found it to be haunted by
ghostly sardonic laughter. As a result it is hardly habitable. There is hope, however,
for by actual testing I have found that this haunting is subject to certain laws, obscure
but infallible, and that the laughter can be affected by my playing the organ or burning
incense. In each minute, the laughter occurs or not, it shows no degree. What it will
do during the ensuing minute depends, in the following exact way, on what has been
happening during the preceding minute: Whenever there is laughter, it will continue in
the succeeding minute unless I play the organ, in which case it will stop. But continuing
to play the organ does not keep the house quiet. I notice, however, that whenever I
burn incense when the house is quiet and do not play the organ it remains quiet for the
next minute. At this minute of writing, the laughter is going on. Please tell me what
manipulations of incense and organ I should make to get that house quiet, and to keep
it so.\\
Sincerely,\\
At Wits End
\begin{enumerate}[label=(\alph*)]
    \item (3 marks) Formulate this problem as an MDP (for the sake of uniformity, formulate it as a
continuing discounted problem, with $\gamma= 0.9$. Let the reward be +1 on any transition
into the silent state, and -1 on any transition into the laughing state.) Explicitly give the
state set, action sets, state transition, and reward function.
\begin{solution}
%Write solution here
\end{solution}

    \item (2 marks) Starting with simple policy of \textbf{always} burning incense, and not playing organ, perform a couple of policy iterations.
\begin{solution}
%Write solution here
\end{solution}
    
    \item (2 marks) Finally, what is your advice to "At Wits End"?
\begin{solution}
%Write solution here
\end{solution}
    
    
\end{enumerate}

\question[4] Consider the task of controlling a system when the control actions are delayed. The control agent takes an action on observing the state at time $t$. The action is applied to the system at time $t + \tau$. The agent
receives a reward at each time step.
\begin{enumerate}[label=(\alph*)]
\item (2 marks)What is an appropriate notion of return for this task?
\begin{solution}
%Write solution here
\end{solution}
\item (2 marks) Give the TD(0) backup equation for estimating the value function of a given policy.
\begin{solution}
%Write solution here
\end{solution}
\end{enumerate}
\end{questions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%THE END%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}