\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Hierarchical Reinforcement Learning}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Answers-1: Grid World of Four Rooms and Visualization the learned Q values}{1}{subsection.1.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{grid_a_arrow}{{1(a)}{1}{Subfigure 1(a)}{subfigure.1.1}{}}
\newlabel{sub@grid_a_arrow}{{(a)}{1}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{grid_b_arrow}{{1(b)}{1}{Subfigure 1(b)}{subfigure.1.2}{}}
\newlabel{sub@grid_b_arrow}{{(b)}{1}{Subfigure 1(b)\relax }{subfigure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Grid world of four rooms. Blue:Agent, Green:Terminal The grid world-\ref  {grid_a_arrow} has terminal state G1 and grid world-\ref  {grid_b_arrow} has terminal state G2. Arrow indicates the optimal policy. The policy in fig-\ref  {grid_a_arrow} is obtained using option-1 where as same in fig.-\ref  {grid_b_arrow} by option-2 \relax }}{1}{figure.caption.1}}
\newlabel{fig:grids}{{1}{1}{Grid world of four rooms. Blue:Agent, Green:Terminal The grid world-\ref {grid_a_arrow} has terminal state G1 and grid world-\ref {grid_b_arrow} has terminal state G2. Arrow indicates the optimal policy. The policy in fig-\ref {grid_a_arrow} is obtained using option-1 where as same in fig.-\ref {grid_b_arrow} by option-2 \relax }{figure.caption.1}{}}
\newlabel{grid_a_circle}{{2(a)}{2}{Subfigure 2(a)}{subfigure.2.1}{}}
\newlabel{sub@grid_a_circle}{{(a)}{2}{Subfigure 2(a)\relax }{subfigure.2.1}{}}
\newlabel{grid_b_circle}{{2(b)}{2}{Subfigure 2(b)}{subfigure.2.2}{}}
\newlabel{sub@grid_b_circle}{{(b)}{2}{Subfigure 2(b)\relax }{subfigure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The grid world-\ref  {grid_a_circle} has terminal state G1 and grid world-\ref  {grid_b_circle} has terminal state G2. Circle size indicates the associated state value. The values in fig-\ref  {grid_a_circle} is obtained using option-1 where as same in fig.-\ref  {grid_b_circle} by option-2\relax }}{2}{figure.caption.2}}
\newlabel{fig:state_val}{{2}{2}{The grid world-\ref {grid_a_circle} has terminal state G1 and grid world-\ref {grid_b_circle} has terminal state G2. Circle size indicates the associated state value. The values in fig-\ref {grid_a_circle} is obtained using option-1 where as same in fig.-\ref {grid_b_circle} by option-2\relax }{figure.caption.2}{}}
\newlabel{SMDP_G1_policy}{{3(a)}{3}{Subfigure 3(a)}{subfigure.3.1}{}}
\newlabel{sub@SMDP_G1_policy}{{(a)}{3}{Subfigure 3(a)\relax }{subfigure.3.1}{}}
\newlabel{SMDP_G1_value}{{3(b)}{3}{Subfigure 3(b)}{subfigure.3.2}{}}
\newlabel{sub@SMDP_G1_value}{{(b)}{3}{Subfigure 3(b)\relax }{subfigure.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref  {SMDP_G1_policy} and associated state values-\ref  {SMDP_G1_value} for Goal-G1 after training of 10000 episodes.\relax }}{3}{figure.caption.3}}
\newlabel{fig:SMDP_G1}{{3}{3}{Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref {SMDP_G1_policy} and associated state values-\ref {SMDP_G1_value} for Goal-G1 after training of 10000 episodes.\relax }{figure.caption.3}{}}
\newlabel{SMDP_G2_policy}{{4(a)}{3}{Subfigure 4(a)}{subfigure.4.1}{}}
\newlabel{sub@SMDP_G2_policy}{{(a)}{3}{Subfigure 4(a)\relax }{subfigure.4.1}{}}
\newlabel{SMDP_G2_value}{{4(b)}{3}{Subfigure 4(b)}{subfigure.4.2}{}}
\newlabel{sub@SMDP_G2_value}{{(b)}{3}{Subfigure 4(b)\relax }{subfigure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref  {SMDP_G2_policy} and associated state values-\ref  {SMDP_G2_value} for Goal-G2 after training of 10000 episodes.\relax }}{3}{figure.caption.4}}
\newlabel{fig:SMDP_G2}{{4}{3}{Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref {SMDP_G2_policy} and associated state values-\ref {SMDP_G2_value} for Goal-G2 after training of 10000 episodes.\relax }{figure.caption.4}{}}
\citation{david}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Answers-2 : Changed initial state to the centre of room 4}{4}{subsection.1.2}}
\newlabel{R4_SMDP_G1_policy}{{5(a)}{4}{Subfigure 5(a)}{subfigure.5.1}{}}
\newlabel{sub@R4_SMDP_G1_policy}{{(a)}{4}{Subfigure 5(a)\relax }{subfigure.5.1}{}}
\newlabel{R4_SMDP_G1_value}{{5(b)}{4}{Subfigure 5(b)}{subfigure.5.2}{}}
\newlabel{sub@R4_SMDP_G1_value}{{(b)}{4}{Subfigure 5(b)\relax }{subfigure.5.2}{}}
\newlabel{R4_SMDP_G2_policy}{{5(c)}{4}{Subfigure 5(c)}{subfigure.5.3}{}}
\newlabel{sub@R4_SMDP_G2_policy}{{(c)}{4}{Subfigure 5(c)\relax }{subfigure.5.3}{}}
\newlabel{R4_SMDP_G2_value}{{5(d)}{4}{Subfigure 5(d)}{subfigure.5.4}{}}
\newlabel{sub@R4_SMDP_G2_value}{{(d)}{4}{Subfigure 5(d)\relax }{subfigure.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref  {R4_SMDP_G1_policy} and associated state values-\ref  {R4_SMDP_G1_value} for Goal-G1 after training of 10000 episodes. Same way, the optimal policy-\ref  {R4_SMDP_G2_policy} and associated state values-\ref  {R4_SMDP_G2_value} for Goal-G2\relax }}{4}{figure.caption.5}}
\newlabel{fig:R4_SMDP}{{5}{4}{Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref {R4_SMDP_G1_policy} and associated state values-\ref {R4_SMDP_G1_value} for Goal-G1 after training of 10000 episodes. Same way, the optimal policy-\ref {R4_SMDP_G2_policy} and associated state values-\ref {R4_SMDP_G2_value} for Goal-G2\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Answers-3: Intra-option Q learning}{5}{subsection.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Deep Reinforcement Learning:}{5}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Answers-1: Hyper-parameter Tuning}{5}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Answers-2: Report of hyper-parameters tuning}{5}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Answers-3: Report of the variation of hyper-parameters like hidden layer sizes, epsilon, mini-batch size, target frequency.}{5}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Answers-4: Observations and inferences of removal of the experience replay and/or the target network}{5}{subsection.2.4}}
\bibstyle{ieeetr}
\bibdata{biblio.bib}
\bibcite{david}{{1}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
