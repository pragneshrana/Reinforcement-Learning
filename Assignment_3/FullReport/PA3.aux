\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Hierarchical Reinforcement Learning}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Answers-1: Grid World of Four Rooms and Visualization the learned Q values}{1}{subsection.1.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{grid_a_arrow}{{1(a)}{2}{Subfigure 1(a)}{subfigure.1.1}{}}
\newlabel{sub@grid_a_arrow}{{(a)}{2}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{grid_b_arrow}{{1(b)}{2}{Subfigure 1(b)}{subfigure.1.2}{}}
\newlabel{sub@grid_b_arrow}{{(b)}{2}{Subfigure 1(b)\relax }{subfigure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Grid world of four rooms. Blue:Agent, Green:Terminal The grid world-\ref  {grid_a_arrow} has terminal state G1 and grid world-\ref  {grid_b_arrow} has terminal state G2. Arrow indicates the optimal policy. The policy in fig-\ref  {grid_a_arrow} is obtained using option-1 where as same in fig.-\ref  {grid_b_arrow} by option-2 \relax }}{2}{figure.caption.1}}
\newlabel{fig:grids}{{1}{2}{Grid world of four rooms. Blue:Agent, Green:Terminal The grid world-\ref {grid_a_arrow} has terminal state G1 and grid world-\ref {grid_b_arrow} has terminal state G2. Arrow indicates the optimal policy. The policy in fig-\ref {grid_a_arrow} is obtained using option-1 where as same in fig.-\ref {grid_b_arrow} by option-2 \relax }{figure.caption.1}{}}
\newlabel{grid_a_circle}{{2(a)}{2}{Subfigure 2(a)}{subfigure.2.1}{}}
\newlabel{sub@grid_a_circle}{{(a)}{2}{Subfigure 2(a)\relax }{subfigure.2.1}{}}
\newlabel{grid_b_circle}{{2(b)}{2}{Subfigure 2(b)}{subfigure.2.2}{}}
\newlabel{sub@grid_b_circle}{{(b)}{2}{Subfigure 2(b)\relax }{subfigure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The grid world-\ref  {grid_a_circle} has terminal state G1 and grid world-\ref  {grid_b_circle} has terminal state G2. Circle size indicates the associated state value. The values in fig-\ref  {grid_a_circle} is obtained using option-1 where as same in fig.-\ref  {grid_b_circle} by option-2\relax }}{2}{figure.caption.2}}
\newlabel{fig:state_val}{{2}{2}{The grid world-\ref {grid_a_circle} has terminal state G1 and grid world-\ref {grid_b_circle} has terminal state G2. Circle size indicates the associated state value. The values in fig-\ref {grid_a_circle} is obtained using option-1 where as same in fig.-\ref {grid_b_circle} by option-2\relax }{figure.caption.2}{}}
\newlabel{SMDP_G1_policy}{{3(a)}{3}{Subfigure 3(a)}{subfigure.3.1}{}}
\newlabel{sub@SMDP_G1_policy}{{(a)}{3}{Subfigure 3(a)\relax }{subfigure.3.1}{}}
\newlabel{SMDP_G1_value}{{3(b)}{3}{Subfigure 3(b)}{subfigure.3.2}{}}
\newlabel{sub@SMDP_G1_value}{{(b)}{3}{Subfigure 3(b)\relax }{subfigure.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref  {SMDP_G1_policy} and associated state values-\ref  {SMDP_G1_value} for Goal-G1 after training of 10000 episodes.(By SMDP)\relax }}{3}{figure.caption.3}}
\newlabel{fig:SMDP_G1}{{3}{3}{Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref {SMDP_G1_policy} and associated state values-\ref {SMDP_G1_value} for Goal-G1 after training of 10000 episodes.(By SMDP)\relax }{figure.caption.3}{}}
\newlabel{SMDP_G2_policy}{{4(a)}{4}{Subfigure 4(a)}{subfigure.4.1}{}}
\newlabel{sub@SMDP_G2_policy}{{(a)}{4}{Subfigure 4(a)\relax }{subfigure.4.1}{}}
\newlabel{SMDP_G2_value}{{4(b)}{4}{Subfigure 4(b)}{subfigure.4.2}{}}
\newlabel{sub@SMDP_G2_value}{{(b)}{4}{Subfigure 4(b)\relax }{subfigure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref  {SMDP_G2_policy} and associated state values-\ref  {SMDP_G2_value} for Goal-G2 after training of 10000 episodes.(By SMDP)\relax }}{4}{figure.caption.4}}
\newlabel{fig:SMDP_G2}{{4}{4}{Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref {SMDP_G2_policy} and associated state values-\ref {SMDP_G2_value} for Goal-G2 after training of 10000 episodes.(By SMDP)\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Answers-2 : Changed initial state to the centre of room 4}{5}{subsection.1.2}}
\newlabel{R4_SMDP_G1_policy}{{5(a)}{5}{Subfigure 5(a)}{subfigure.5.1}{}}
\newlabel{sub@R4_SMDP_G1_policy}{{(a)}{5}{Subfigure 5(a)\relax }{subfigure.5.1}{}}
\newlabel{R4_SMDP_G1_value}{{5(b)}{5}{Subfigure 5(b)}{subfigure.5.2}{}}
\newlabel{sub@R4_SMDP_G1_value}{{(b)}{5}{Subfigure 5(b)\relax }{subfigure.5.2}{}}
\newlabel{R4_SMDP_G2_policy}{{5(c)}{5}{Subfigure 5(c)}{subfigure.5.3}{}}
\newlabel{sub@R4_SMDP_G2_policy}{{(c)}{5}{Subfigure 5(c)\relax }{subfigure.5.3}{}}
\newlabel{R4_SMDP_G2_value}{{5(d)}{5}{Subfigure 5(d)}{subfigure.5.4}{}}
\newlabel{sub@R4_SMDP_G2_value}{{(d)}{5}{Subfigure 5(d)\relax }{subfigure.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref  {R4_SMDP_G1_policy} and associated state values-\ref  {R4_SMDP_G1_value} for Goal-G1 after training of 10000 episodes. Same way, the optimal policy-\ref  {R4_SMDP_G2_policy} and associated state values-\ref  {R4_SMDP_G2_value} for Goal-G2 (By SMDP) \relax }}{5}{figure.caption.5}}
\newlabel{fig:R4_SMDP}{{5}{5}{Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref {R4_SMDP_G1_policy} and associated state values-\ref {R4_SMDP_G1_value} for Goal-G1 after training of 10000 episodes. Same way, the optimal policy-\ref {R4_SMDP_G2_policy} and associated state values-\ref {R4_SMDP_G2_value} for Goal-G2 (By SMDP) \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Bonus Answers-3: Intra-option Q learning}{6}{subsection.1.3}}
\newlabel{R1_Intra_G1_policy}{{6(a)}{6}{Subfigure 6(a)}{subfigure.6.1}{}}
\newlabel{sub@R1_Intra_G1_policy}{{(a)}{6}{Subfigure 6(a)\relax }{subfigure.6.1}{}}
\newlabel{R1_Intra_G1_value}{{6(b)}{6}{Subfigure 6(b)}{subfigure.6.2}{}}
\newlabel{sub@R1_Intra_G1_value}{{(b)}{6}{Subfigure 6(b)\relax }{subfigure.6.2}{}}
\newlabel{R1_Intra_G2_policy}{{6(c)}{6}{Subfigure 6(c)}{subfigure.6.3}{}}
\newlabel{sub@R1_Intra_G2_policy}{{(c)}{6}{Subfigure 6(c)\relax }{subfigure.6.3}{}}
\newlabel{R1_Intra_G2_value}{{6(d)}{6}{Subfigure 6(d)}{subfigure.6.4}{}}
\newlabel{sub@R1_Intra_G2_value}{{(d)}{6}{Subfigure 6(d)\relax }{subfigure.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref  {R1_Intra_G1_policy} and associated state values-\ref  {R1_Intra_G1_value} for Goal-G1 after training of 10000 episodes. Same way, the optimal policy-\ref  {R1_Intra_G2_policy} and associated state values-\ref  {R1_Intra_G2_value} for Goal-G2 (By Intra option Q learning)\relax }}{6}{figure.caption.6}}
\newlabel{fig:R1_Intra}{{6}{6}{Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref {R1_Intra_G1_policy} and associated state values-\ref {R1_Intra_G1_value} for Goal-G1 after training of 10000 episodes. Same way, the optimal policy-\ref {R1_Intra_G2_policy} and associated state values-\ref {R1_Intra_G2_value} for Goal-G2 (By Intra option Q learning)\relax }{figure.caption.6}{}}
\newlabel{R4_Intra_G1_policy}{{7(a)}{7}{Subfigure 7(a)}{subfigure.7.1}{}}
\newlabel{sub@R4_Intra_G1_policy}{{(a)}{7}{Subfigure 7(a)\relax }{subfigure.7.1}{}}
\newlabel{R4_Intra_G1_value}{{7(b)}{7}{Subfigure 7(b)}{subfigure.7.2}{}}
\newlabel{sub@R4_Intra_G1_value}{{(b)}{7}{Subfigure 7(b)\relax }{subfigure.7.2}{}}
\newlabel{R4_Intra_G2_policy}{{7(c)}{7}{Subfigure 7(c)}{subfigure.7.3}{}}
\newlabel{sub@R4_Intra_G2_policy}{{(c)}{7}{Subfigure 7(c)\relax }{subfigure.7.3}{}}
\newlabel{R4_Intra_G2_value}{{7(d)}{7}{Subfigure 7(d)}{subfigure.7.4}{}}
\newlabel{sub@R4_Intra_G2_value}{{(d)}{7}{Subfigure 7(d)\relax }{subfigure.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref  {R4_Intra_G1_policy} and associated state values-\ref  {R4_Intra_G1_value} for Goal-G1 after training of 10000 episodes. Same way, the optimal policy-\ref  {R4_Intra_G2_policy} and associated state values-\ref  {R4_Intra_G2_value} for Goal-G2 (By Intra option Q learning)\relax }}{7}{figure.caption.7}}
\newlabel{fig:R4_Intra}{{7}{7}{Grid world of four rooms. Blue:Agent, Green:Terminal. The optimal policy-\ref {R4_Intra_G1_policy} and associated state values-\ref {R4_Intra_G1_value} for Goal-G1 after training of 10000 episodes. Same way, the optimal policy-\ref {R4_Intra_G2_policy} and associated state values-\ref {R4_Intra_G2_value} for Goal-G2 (By Intra option Q learning)\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Learning Curves of Average Return and Steps :}{8}{subsection.1.4}}
\newlabel{avg_steps_G1}{{8(a)}{8}{Subfigure 8(a)}{subfigure.8.1}{}}
\newlabel{sub@avg_steps_G1}{{(a)}{8}{Subfigure 8(a)\relax }{subfigure.8.1}{}}
\newlabel{avg_steps_G2}{{8(b)}{8}{Subfigure 8(b)}{subfigure.8.2}{}}
\newlabel{sub@avg_steps_G2}{{(b)}{8}{Subfigure 8(b)\relax }{subfigure.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Total number of average steps in case of initiation of agent at different states for SMDP and Intra-Option Q learning for goal-G1 \ref  {avg_steps_G1} and for goal-G2-\ref  {avg_steps_G2}\relax }}{8}{figure.caption.8}}
\newlabel{fig:Avg_stes}{{8}{8}{Total number of average steps in case of initiation of agent at different states for SMDP and Intra-Option Q learning for goal-G1 \ref {avg_steps_G1} and for goal-G2-\ref {avg_steps_G2}\relax }{figure.caption.8}{}}
\newlabel{action_G1}{{9(a)}{8}{Subfigure 9(a)}{subfigure.9.1}{}}
\newlabel{sub@action_G1}{{(a)}{8}{Subfigure 9(a)\relax }{subfigure.9.1}{}}
\newlabel{action_G2}{{9(b)}{8}{Subfigure 9(b)}{subfigure.9.2}{}}
\newlabel{sub@action_G2}{{(b)}{8}{Subfigure 9(b)\relax }{subfigure.9.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Average total return by SMDP and Intra-option Q learning for goal-G1 \ref  {avg_steps_G1} and for goal-G2-\ref  {avg_steps_G2}. Each primitive action denotes the single step. The average is taken over 10 episodes and x-axis follows $\qopname  \relax o{log}10$ scale.\relax }}{8}{figure.caption.9}}
\newlabel{fig:action}{{9}{8}{Average total return by SMDP and Intra-option Q learning for goal-G1 \ref {avg_steps_G1} and for goal-G2-\ref {avg_steps_G2}. Each primitive action denotes the single step. The average is taken over 10 episodes and x-axis follows $\log 10$ scale.\relax }{figure.caption.9}{}}
\newlabel{option_G1}{{10(a)}{9}{Subfigure 10(a)}{subfigure.10.1}{}}
\newlabel{sub@option_G1}{{(a)}{9}{Subfigure 10(a)\relax }{subfigure.10.1}{}}
\newlabel{option_G2}{{10(b)}{9}{Subfigure 10(b)}{subfigure.10.2}{}}
\newlabel{sub@option_G2}{{(b)}{9}{Subfigure 10(b)\relax }{subfigure.10.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Average total return by SMDP and Intra-option Q learning for goal-G1 \ref  {avg_steps_G1} and for goal-G2-\ref  {avg_steps_G2}. Each option denotes the single step. The average is taken over 10 episodes and x-axis follows $\qopname  \relax o{log}10$ scale.\relax }}{9}{figure.caption.10}}
\newlabel{fig:options}{{10}{9}{Average total return by SMDP and Intra-option Q learning for goal-G1 \ref {avg_steps_G1} and for goal-G2-\ref {avg_steps_G2}. Each option denotes the single step. The average is taken over 10 episodes and x-axis follows $\log 10$ scale.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Deep Reinforcement Learning:}{10}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Answers-1: Hyper-parameters and Learning Curve}{10}{subsection.2.1}}
\newlabel{re_simple_DQN}{{11(a)}{10}{Subfigure 11(a)}{subfigure.11.1}{}}
\newlabel{sub@re_simple_DQN}{{(a)}{10}{Subfigure 11(a)\relax }{subfigure.11.1}{}}
\newlabel{epi_simple_DQN}{{11(b)}{10}{Subfigure 11(b)}{subfigure.11.2}{}}
\newlabel{sub@epi_simple_DQN}{{(b)}{10}{Subfigure 11(b)\relax }{subfigure.11.2}{}}
\newlabel{ler_simple_DQN}{{11(c)}{10}{Subfigure 11(c)}{subfigure.11.3}{}}
\newlabel{sub@ler_simple_DQN}{{(c)}{10}{Subfigure 11(c)\relax }{subfigure.11.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Plots of Average total reward of last 100 episode \ref  {re_simple_DQN}, episode length \ref  {epi_simple_DQN}, learned agent episode length \ref  {ler_simple_DQN}\relax }}{10}{figure.caption.11}}
\newlabel{fig:simple}{{11}{10}{Plots of Average total reward of last 100 episode \ref {re_simple_DQN}, episode length \ref {epi_simple_DQN}, learned agent episode length \ref {ler_simple_DQN}\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Answers-2: Report of hyper-parameters tuning}{11}{subsection.2.2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Set of Hyperparameters\relax }}{11}{table.caption.12}}
\newlabel{tab:hyperPara}{{1}{11}{Set of Hyperparameters\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Answers-3: Report of the variation of hyper-parameters like hidden layer sizes, epsilon, mini-batch size, target frequency.}{12}{subsection.2.3}}
\newlabel{epsilon}{{12(a)}{12}{Subfigure 12(a)}{subfigure.12.1}{}}
\newlabel{sub@epsilon}{{(a)}{12}{Subfigure 12(a)\relax }{subfigure.12.1}{}}
\newlabel{hidden}{{12(b)}{12}{Subfigure 12(b)}{subfigure.12.2}{}}
\newlabel{sub@hidden}{{(b)}{12}{Subfigure 12(b)\relax }{subfigure.12.2}{}}
\newlabel{minibatch}{{12(c)}{12}{Subfigure 12(c)}{subfigure.12.3}{}}
\newlabel{sub@minibatch}{{(c)}{12}{Subfigure 12(c)\relax }{subfigure.12.3}{}}
\newlabel{target}{{12(d)}{12}{Subfigure 12(d)}{subfigure.12.4}{}}
\newlabel{sub@target}{{(d)}{12}{Subfigure 12(d)\relax }{subfigure.12.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Comparative plots of learning curves of different hyper parameters : Initial value of epsilon-\ref  {epsilon}, Hidden layers-\ref  {hidden}, size of minibatch-\ref  {minibatch}, update frequency of target networks-\ref  {target}\relax }}{12}{figure.caption.13}}
\newlabel{hyper}{{12}{12}{Comparative plots of learning curves of different hyper parameters : Initial value of epsilon-\ref {epsilon}, Hidden layers-\ref {hidden}, size of minibatch-\ref {minibatch}, update frequency of target networks-\ref {target}\relax }{figure.caption.13}{}}
\newlabel{epsilon_len}{{13(a)}{13}{Subfigure 13(a)}{subfigure.13.1}{}}
\newlabel{sub@epsilon_len}{{(a)}{13}{Subfigure 13(a)\relax }{subfigure.13.1}{}}
\newlabel{hidden_len}{{13(b)}{13}{Subfigure 13(b)}{subfigure.13.2}{}}
\newlabel{sub@hidden_len}{{(b)}{13}{Subfigure 13(b)\relax }{subfigure.13.2}{}}
\newlabel{minibatch_len}{{13(c)}{13}{Subfigure 13(c)}{subfigure.13.3}{}}
\newlabel{sub@minibatch_len}{{(c)}{13}{Subfigure 13(c)\relax }{subfigure.13.3}{}}
\newlabel{target_len}{{13(d)}{13}{Subfigure 13(d)}{subfigure.13.4}{}}
\newlabel{sub@target_len}{{(d)}{13}{Subfigure 13(d)\relax }{subfigure.13.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Comparative Plots of episode length of different hyper parameters : Initial value of epsilon-\ref  {epsilon}, Hidden layers-\ref  {hidden}, size of minibatch-\ref  {minibatch}, update frequency of target networks-\ref  {target}\relax }}{13}{figure.caption.14}}
\newlabel{fig:episo_len}{{13}{13}{Comparative Plots of episode length of different hyper parameters : Initial value of epsilon-\ref {epsilon}, Hidden layers-\ref {hidden}, size of minibatch-\ref {minibatch}, update frequency of target networks-\ref {target}\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Bonus Answers-4: Observations and inferences of removal of the experience replay and/or the target network}{14}{subsection.2.4}}
\newlabel{reward_wos}{{14(a)}{14}{Subfigure 14(a)}{subfigure.14.1}{}}
\newlabel{sub@reward_wos}{{(a)}{14}{Subfigure 14(a)\relax }{subfigure.14.1}{}}
\newlabel{len_wos}{{14(b)}{14}{Subfigure 14(b)}{subfigure.14.2}{}}
\newlabel{sub@len_wos}{{(b)}{14}{Subfigure 14(b)\relax }{subfigure.14.2}{}}
\newlabel{epi_wos}{{14(c)}{14}{Subfigure 14(c)}{subfigure.14.3}{}}
\newlabel{sub@epi_wos}{{(c)}{14}{Subfigure 14(c)\relax }{subfigure.14.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Plots of average reward-\ref  {reward_wos}, length of episodes-\ref  {epi_wos}, episode length for leaned agent-\ref  {len_wos} after removing the target network, experience reply and both together. \relax }}{14}{figure.caption.15}}
\newlabel{fig:without}{{14}{14}{Plots of average reward-\ref {reward_wos}, length of episodes-\ref {epi_wos}, episode length for leaned agent-\ref {len_wos} after removing the target network, experience reply and both together. \relax }{figure.caption.15}{}}
\bibstyle{ieeetr}
\bibdata{biblio.bib}
\bibcite{david}{{1}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
