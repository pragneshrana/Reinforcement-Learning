\documentclass[solution,addpoints,12pt]{exam}
\printanswers
\usepackage{amsmath,amssymb}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue]{hyperref}
\newcommand{\RP}{\ensuremath{\mathsf{RP}}}
\newcommand{\expect}[1]{\ensuremath{\mathbb{E}[#1]}}
\newcommand{\dx}{\mathrm{d}x}
\usepackage{graphicx}
\begin{document}

\hrule
\vspace{1mm}
\noindent 
\begin{center}
{\Large CS6700 : Reinforcement Learning} \\
{\large Written Assignment \#2}
\end{center}
%\hfill Release date: 21 Jan, 2017, 12:00 pm}
\vspace{1mm}
\noindent 
{Deadline: 30-May-2020}

\vspace{2mm}
\hrule

{\small

\begin{itemize}\itemsep0mm
\item This is an individual assignment. Collaborations and discussions are strictly
prohibited.
\item Be precise with your explanations. Unnecessary verbosity will be penalized.
\item Check the Moodle discussion forums regularly for updates regarding the assignment.
\item \textbf{Please start early.}

\end{itemize}
}

\hrule

\vspace{3mm}
\noindent {\sc Author : Rana Pragneshkumar Rajubhai} \\[1mm]
\noindent {\sc Roll Number : ME17S301} \\
\hrule


\begin{questions}
\question[3]
Consider a bandit problem in which the policy parameters are mean $ \mu$ and variance $\sigma$ of normal distribution according to which actions are selected. Policy is defined as $ \pi(a;\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma}}e^-\frac{(a-\mu)^2}{2\sigma^2}$. Derive the parameter update conditions according to the REINFORCE procedure (assume baseline is zero).

\begin{solution}
The REINFORCEMENT parameter update equation can be written as,\\
\begin{equation}
	\Delta \theta_n = \alpha (R_n - b_n) \frac{\partial \ln \pi(a_n;\theta)}{\partial \theta_n}
\end{equation}
The parameters for Gaussian policy is given as $\mu$ and $\sigma$ and policy is given as,
\begin{equation}
	\pi(a;\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma}}e^-\frac{(a-\mu)^2}{2\sigma^2}
\end{equation}
Update of $\mu$:
\begin{equation}
\begin{aligned}
\frac{\partial \ln \pi(a_n;\mu_n,\sigma_n)}{\partial \mu_n} &= \frac{\partial}{\partial\mu_n} \bigg\{ \frac{-(a_n -\mu_n)^2}{2\sigma_n^2}\bigg\} 
= \frac{a_n-\mu_n}{\sigma_n^2}
\end{aligned}
\end{equation}
Considering baseline performance to zero(b=0), \\
\begin{equation}
	\text{\textbf{update rule for mean  :   }  }
	\mu_{n+1} = \mu_{n} + \alpha R_n \bigg(\frac{a_n-\mu_n}{\sigma_n^2}\bigg)
\end{equation}
similarly,
Update of variance($\sigma$):
\begin{equation}
\begin{aligned}
\frac{\partial \ln \pi(a_n;\mu_n,\sigma_n)}{\partial \sigma_n} &= \frac{\partial}{\partial\sigma_n} \bigg\{ -\ln (\sqrt{2\pi\sigma_n})\bigg\}+ \frac{\partial}{\partial \sigma_n}\bigg\{- \frac{-(a_n -\mu_n)^2}{2\sigma_n^2} \bigg\}\\
&= - \frac{1}{\sigma_n} +\frac{-(a_n -\mu_n)^2}{2\sigma_n^2}
= \frac{1}{\sigma_n} \bigg\{ \bigg(\frac{a_n -\mu_n}{\sigma_n}\bigg)^2 - 1 \bigg\}
\end{aligned}
\end{equation}
\begin{equation}
\text{\textbf{Update rule for variance  :}      } 
\sigma_{n+1} = \sigma_{n} + \alpha R_n \frac{1}{\sigma_n} \bigg\{ \bigg(\frac{a_n -\mu_n}{\sigma_n}\bigg)^2 - 1 \bigg\}
\end{equation}


\end{solution}
\question[6]
 Let us consider the effect of approximation on policy search and value function based methods. Suppose that a policy gradient method uses a class of policies that do not contain the optimal policy; and a value function based method uses a function appropriator that can represent the values of the policies of this class, but not that of the optimal policy.
 \begin{enumerate}[label=(\alph*)]
     \question[2]  Why would you consider the policy gradient approach to be better than the value function based approach?
     \begin{solution}
		Value function based methods approximation the Q function and use to find optimal policy so, in case of non-existence of optimal policy, it may get stuck infinitely in loop or it might take all the upper defined limit of iteration. Whereas, the policy gradient has better convergence criteria as it directly optimize the policy space by finding the local optimal policy. The action is picked based on the state which maximized the reward which done using parameter$\theta$.
		\begin{equation}
			\pi(a|a,\theta = \Pr{A_t=a,S_t,\theta_t=\theta})
		\end{equation}
		Eq.- shows that, the policy is denotes the probability of taking action based on state(s) and parameter($\theta$).
		
		 Due to this characteristic, policy gradient method may converge to local optima or best action are more likely to get sampled and converges. Due to this, policy gradient approach is better than value function approximation. 
     
     \end{solution}
     \question[2]  Under what circumstances would the value function based approach be better than the policy gradient approach?
     \begin{solution}
     For the case of existence of optimal policy, value function based approximation technique is useful as policy gradient method may end up with sub optimal policy.  Even value function method works better for high variance case. In case of high variance policy gradient requires other variance reduction treatment.
     \end{solution}
     \question[2]  Is there some circumstance under which either of the method can find the optimal policy?
     \begin{solution}
     	Yes, in the case of deterministic policy value function works and the policy gradient may converge the best sub-optimal policy. If optima exist, then both method can find optimal learn optimal policy and policy gradient can do it efficiently.  
     \end{solution}
 
 \end{enumerate}
 
 \newpage

\question[4] Answer the following questions with respect to the DQN algorithm:
\begin{itemize}
    \question [2] When using one-step TD backup, the TD target is $R_{t+1}+\gamma V(S_{t+1},\theta)$ and the update to the neural network parameter is as follows:\\
    \begin{equation}
        \Delta \theta=\alpha(R_{t+1}+\gamma V(S_{t+1},\theta)-V(S_{t},\theta))\nabla_{\theta}V(S_{t},\theta)
    \end{equation}
      Is the update correct ? Is any term missing ? Justify your answer
    \begin{solution}
    	\item Yes, the update is correct
    	\item No term is missing.
    	
    	\begin{equation}
    	\Delta \theta=\alpha(R_{t+1}+\gamma V(S_{t+1},\theta)-V(S_{t},\theta))\nabla_{\theta}V(S_{t},\theta)
    	\end{equation}
    	
    	\item For the given equation,\\
    	$(R_{t+1}+\gamma V(S_{t+1},\theta)$ denotes the target and $V(S_{t},\theta)$ term is required control function.
    	
    	The update rule is given as,
    	\begin{equation}
    		\theta_{t+1} = \theta_{t} + \alpha \nabla_{\theta_{t}}[(R_{t+1} + \gamma \hat{V}(s_{t},\theta_t) -\hat{V}(s_{t},\theta_t))^2 ]
    	\end{equation}
    	By differentiating w.r.t $\theta$ the target term will act as constant as it can be replace by return like $G_t$ so, completer update rule will be,
    	\begin{equation}
    	\theta_{t+1} = \theta_{t} + \alpha(R_{t+1}+\gamma V(S_{t+1},\theta)-V(S_{t},\theta))\nabla_{\theta}V(S_{t},\theta)
    	\end{equation}
    	
    	
    
    \end{solution}
    \question [2] Describe the two ways discussed in class to update the parameters of target network. Which one is better and why?
    \begin{solution}
    	The two ways to update the parameters mentioned in the lecture is based on the sampling method. As the sampling method varies, the associated data and its up-gradation in parameters also varies.
    	\item Online- off policy algorithm
    	\item Memory based transition reply 
    	
    	Memory based transition reply is better
    	
    	Out of which transition state reply is good as it solves the problem f repeated samples. In case of online algo. the obtained samples might correlated and may create problem in convergence which is not observed in case of transition reply.
    	
    
    \end{solution}
\end{itemize}

\newpage
\question[4] Experience replay is vital for stable training of DQN.
\begin{parts}
    \part[2] What is the role of the experience replay in DQN?
    \begin{solution}
    The neural network can get learn samples from experience which might be consecutive but it might be highly correlated. Such correlation makes learning inefficient. To break such correlation experience reply memory us useful as it stores all the sample and randomly samples will be picked up.
	In short, \textbf{ experience replay memory is useful to break the correlation between consecutive samples.} 
    \end{solution}
    \part[2] Consequent works in literature sample transitions from the experience replay, in proportion to the TD-error. Hence, instead of sampling transitions using a uniform-random strategy, higher TD-error transitions are sampled at a higher frequency. Why would such a modification help?
    \begin{solution}
    The uniform random strategy may cause bias in certain state-action space. Due to repetition or any other reason,  certain random sample might get sampled often. Such problem can be resolved by sampling from higher TD error transition. If samples from \textbf{certain state-action space} is not picked up from long duration then it may caused more error so, sampling based on TD error in next few iteration with higher frequency can stabilize the process. Eventually, \textbf{it will cover whole region and sampling transition will be smooth rather than abrupt.}
    \end{solution}
\end{parts}

\question[3] We discussed two different motivations for actor-critic algorithms: the original motivation was as an extension of reinforcement comparison, and the modern motivation is as a variance reduction mechanism for policy gradient algorithms. Why is the original version
of actor-critic not a policy gradient method?
\begin{solution}
The reinforce can be written as,
\begin{equation}
\begin{aligned}
\Delta \theta &= \alpha_n (r_n - b_n) \frac{\partial \ln \pi(a,\theta_n)}{\partial \theta}\\
&= \alpha_n (G_n - b_n) \frac{\partial \ln \pi(a,\theta_n)}{\partial \theta}
\end{aligned}
\end{equation}
$r_n$ is related to return and can be replace by $G_n$.

The original version of reinforce contains $G_n$ term in  the formulation  which causes high variance in the update as each time different sample might have different return. 
If the $G_n$ term is replaced by the value function or some form of value function then variation in that term can be reduced which connects the reinforce to policy gradient. So, the original reinforce or actor critic in not policy gradient.
\end{solution}


\question[4]
This question requires you to do some \href{https://arxiv.org/abs/cs/9905014}{additional reading}.  Dietterich specifies certain conditions for safe-state abstraction for the MaxQ framework.  I had mentioned in class that even if we do not use the MaxQ value function decomposition, the hierarchy provided is still useful.  So, which of the safe-state abstraction conditions are still necessary when we do not use value function decomposition?
\begin{solution}
The required five condition are mentioned below that permits "safe" introduction of state abstraction.
\subitem \textbf{Condition 1: Max Node Irrelevance} \\
 Useful when a set of state variables is irrelevant to Max Node. It should satisfy the following properties:
 \subsubitem $->$ The state transition probability distribution $P^\pi$(s',N|s,a) at node i can be factored into the product of two distribution.
 \begin{equation}
 	P^\pi(x',y',N|x,y,a) = P^\pi(y'|y,a) P^\pi(x',N|x,a)
 \end{equation}
 \subsubitem $->$ for any pair of the states $s_1 = (x,y_1)$ and $s_2 = (x,y_2)$ such that $\xi(s1)$ = $\xi(s2)$ = x and for any child action a, $V^\pi(a,s_1) = V^\pi(a,s_1)$ and $R_i(s_1) = R_i(s_2)$
\subitem  \textbf{Condition 2: Leaf Irrelevance} \\ This condition describes the situation under which it is possible to apply the state abstraction to leaf nodes of the MAXQ graphs. 
\begin{equation}
\begin{aligned}
	V(a,s) &= \sum_{s'} P(s'|s,a) R(s'|s,a) \\
	\text{and} \\
	\sum_{s'_1} P(s'_1|s,a) R(s'_1|s_1,a) &= \sum_{s'_2} P(s'_2|s_2,a) R(s'_2|s_2,a)
\end{aligned}
\end{equation}
\subitem \textbf{Condition 3: Result Distribution Irrelevance} \\
The mentioned condition is result of funnel condition.
\subsubitem $->$ A set of state variable $Y_j$ is irrelevant for the result distribution of action j if, for all the abstract policies $\pi$ executed by node j and its desccent in the MAXQ hirarchy, the following holds. for all the pair of state $s_1$ and $s_2$ that differ only in their values for the state variables in $Y_j$.
\begin{equation}
	P_\pi(s',N|s_1,j) = P_\pi(s',N|s_2,j) \hspace{3mm} \forall s',N
\end{equation}
\subitem  \textbf{Condition 4: Termination} \\
This condition is also related 'funnel' property. It applies when sub-task is guaranteed to cause its parent task to terminate in a goal state. In a sense, the subtask is funneling the environment into the set of states described by the goal predicate of the parent task. 
\subitem \textbf{Condition 5: Shielding} \\
The shielding condition aries from the structure of MAX Q.  graph. 
The Shielding condition can be verified by analyzing the structure of the MAXQ graph with termination condition.
\end{solution}

\question[3] Consider the problem of solving continuous control tasks using Deep Reinforcement Learning.

\begin{parts}
    \part[2] Why can simple discretization of the action space not be used to solve the problem? In which exact step of the DQN training algorithm is there a problem and why?
    \begin{solution}
	In DQN algorithm, neural network function appropriators were used to estimate the action-value function. DQN can solve problems with high-dimensional observation space an low dimensional, discrete action space. Many real world application have continuous and high dimensional action space. \textbf{DQN cannot be directly apply to continuous domains as it depends on the finding the action that maximize the action-value function. In each step, the action value function in continuous valued case requires as iterative optimization process. }
	
	The exact step in algorithm is,
	\begin{equation}
	\begin{aligned}
\text{target} &= R(s,a,s') + \gamma \max_a' Q_k(s',a') \\
Q_{k+1}(s,a) &\leftarrow (1-\alpha) Q_k(s,a) + \alpha [target]
	\end{aligned}
	\end{equation}
    \end{solution}
    
    \part[1] How is exploration ensured in the DDPG algorithm?
    \begin{solution}
    In DDPG algorithm. while selection of action, noise $\mathcal{N}_t$ is introduced which make sure of exploration.\\
    step:\\
    Select action \textbf{$a_t = \mu(s_t|\theta^\mu) + \mathcal{N}_t$} according to current policy and exploration noise.
    \end{solution}
\end{parts}

\newpage

\question[3] Option discovery has entailed using heuristics, the most popular of which is to identify bottlenecks. Justify why bottlenecks are useful sub-goals. Describe scenarios in which a such a heuristic could fail.
\begin{solution}
Bottle necks are useful as it cut down the cost of \textbf{exploration}. Apart from that, one has to pass through such states which provides the \textbf{re-usability} of known states. In case of transfer of terminal state, bottle neck has some information about previous goal so it helps in\textbf{ transfer learning}.

\textbf{Heuristic failure experience :}
Take the example of two room, many sample path  are obtained. Out which some sample generate successful trajectory whereas, some generate unsuccessful trajectory. In which, the goal is to focus on the trajectory which more successful and less likely to appear in unsuccessful trajectory. 

Such heuristic method shows, certain states has to be covered to reach out the goal.While doing so, it may happen that in certain trajectory it model may take more time and on certain path. For example in room exploration case, agent takes more time one room and eventually it terminates the sample. If agent spends more time in one room then after certain  period the sample will be discarded, which shows the failure of heuristic.
\end{solution}

\end{questions}

\end{document}
