\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{git}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Puddle World:}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Puddle World with different colour: Grey:Start, Pink:Terminal, Middle:Puddle \relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:puddle}{{1}{1}{Puddle World with different colour: Grey:Start, Pink:Terminal, Middle:Puddle \relax }{figure.caption.1}{}}
\citation{sutton2018reinforcement}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Q-learning:}{2}{subsection.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Average steps taken by the agent in different world\relax }}{2}{figure.caption.2}}
\newlabel{fig:stepsQ}{{2}{2}{Average steps taken by the agent in different world\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Average return per episode obtained by the agent in different world for QL\relax }}{3}{figure.caption.3}}
\newlabel{fig:returnQ}{{3}{3}{Average return per episode obtained by the agent in different world for QL\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Combine plot of average return and average steps for different world for QL\relax }}{3}{figure.caption.4}}
\newlabel{fig:combiQ}{{4}{3}{Combine plot of average return and average steps for different world for QL\relax }{figure.caption.4}{}}
\citation{sutton2018reinforcement}
\citation{medium}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Optimal Policy obtained after learning for QL\relax }}{4}{figure.caption.5}}
\newlabel{fig:policyQ}{{5}{4}{Optimal Policy obtained after learning for QL\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}SARSA:}{4}{subsection.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Average steps taken by the agent in different world\relax }}{5}{figure.caption.6}}
\newlabel{fig:stepsS}{{6}{5}{Average steps taken by the agent in different world\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Average return per episode obtained by the agent in different world for SARSA\relax }}{5}{figure.caption.7}}
\newlabel{fig:returnS}{{7}{5}{Average return per episode obtained by the agent in different world for SARSA\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Combine plot of average return and average steps for different world for SARSA\relax }}{6}{figure.caption.8}}
\newlabel{fig:combiS}{{8}{6}{Combine plot of average return and average steps for different world for SARSA\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Optimal Policy obtained after learning for SARSA\relax }}{6}{figure.caption.9}}
\newlabel{fig:policyS}{{9}{6}{Optimal Policy obtained after learning for SARSA\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}SARSA-$\lambda $:}{7}{subsection.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Average return and step with different $\lambda $ value for different puddle world \relax }}{7}{figure.caption.10}}
\newlabel{fig:SLA}{{10}{7}{Average return and step with different $\lambda $ value for different puddle world \relax }{figure.caption.10}{}}
\citation{david}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Average return and step with different $\lambda $ value for different puddle world \relax }}{8}{figure.caption.11}}
\newlabel{fig:SLBC}{{11}{8}{Average return and step with different $\lambda $ value for different puddle world \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Policy Gradient:}{8}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Part:1 The environment implementation}{8}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Part:2 The Rollout Function}{9}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Part:3 Policy Gradient Implementation}{9}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Answers-1 Hyper-parameter Tuning:}{9}{subsection.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Average return per batch size for chakra and vishamC with different hyper-parameters\relax }}{9}{figure.caption.12}}
\newlabel{fig:Preturn}{{12}{9}{Average return per batch size for chakra and vishamC with different hyper-parameters\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Answers-2: Visualization of value function for learned policy}{10}{subsection.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces State value function for learned agent in chakra and vishamC\relax }}{10}{figure.caption.13}}
\newlabel{fig:Pstate1}{{13}{10}{State value function for learned agent in chakra and vishamC\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces State value function for learned agent in chakra and vishamC\relax }}{11}{figure.caption.14}}
\newlabel{fig:Pstate2}{{14}{11}{State value function for learned agent in chakra and vishamC\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Answers-3: Plotting of policy trajectory after learned policy}{11}{subsection.2.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Policy Trajectories for learned agent in chakra and vishamC\relax }}{11}{figure.caption.15}}
\newlabel{fig:Ptraj}{{15}{11}{Policy Trajectories for learned agent in chakra and vishamC\relax }{figure.caption.15}{}}
\bibstyle{ieeetr}
\bibdata{biblio.bib}
\bibcite{git}{{1}{}{{}}{{}}}
\bibcite{sutton2018reinforcement}{{2}{}{{}}{{}}}
\bibcite{medium}{{3}{}{{}}{{}}}
\bibcite{david}{{4}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
