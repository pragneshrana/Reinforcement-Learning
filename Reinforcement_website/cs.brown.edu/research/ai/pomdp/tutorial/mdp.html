<HTML>
<HEADER>
   <META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
   <META NAME="GENERATOR" CONTENT="Mozilla/4.04 [en] (X11; I; Linux 2.1.123 i586) [Netscape]">
   <META NAME="Author" CONTENT="Anthony R. Cassandra">
   <META NAME="Keywords" CONTENT="POMDP, Markov, Planning, decision theory">
   <TITLE>POMDPs for Dummies: Page 2</TITLE>
</HEADER>

<BODY TEXT="#F0F0F0" BGCOLOR="#003300" LINK="#F8D900" VLINK="#CCCCCC" ALINK="#00FF00">

<CENTER><FONT SIZE="-1"><B>
<A HREF="index.html">POMDP Tutorial</A>
| <A HREF="mdp-vi.html">Next</A>
</B></FONT></CENTER>

<P>
<IMG SRC="images/brown-clear.gif" HEIGHT=111 WIDTH=102 ALIGN=LEFT>

<P>
<CENTER><IMG SRC="images/oak-a-stripe.gif" HEIGHT=5 WIDTH=490>
</CENTER>

<CENTER><h1>Brief Introduction to Markov decision processes
(MDPs)</h1></CENTER>

<CENTER><IMG SRC="images/oak-a-stripe.gif"></CENTER>

<P>
When you are confronted with a decision, there are a number of
different alternatives (actions) you have to choose from.  Choosing
the best action requires thinking about more than just the immediate
effects of your actions.  The immediate effects are often easy to see,
but the long term effects are not always as transparent.  Sometimes
actions with poor immediate effects, can have better long term
ramifications. You would like to choose the action that makes the
right tradeoffs between the immediate rewards and the future gains, to
yield the best possible solution.  People do this type of reasoning
daily, and a Markov decision process a way to model problems so that
we can automate this process. <p>

If you can model the problem as an <tt>MDP</tt>, then there are a
number of algorithms that will allow you to automatically solve the
decision problem.  We will first talk about the components of the
model that are required.  We follow this by defining what it means to
solve the problems and finally, we describe a simple algorithm for
finding a solution to the problem given a model of this type.<p>

The four components of an <tt>MDP</tt> model are: a set of states, a
set of actions, the effects of the actions and the immediate value of
the actions.  We now discuss these in more detail.

<h3>States</h3>

When making a decision, you need to think about how your actions will
effect things.  The state is the way the world currently exists and an
action will have the effect of changing the state of the world.  If we
think about the set of every possible way the world could be, then
this is would be the set of state of the world.  Each of these states
would be a state in the <tt>MDP</tt>.

<h3>Actions</h3>

The actions are the set of possible alternatives you can make.  The
problem is to know which of these actions to take in for a particular
state of the world.  

<h3>Transitions</h3>

When we are deciding between different actions, we have some idea of
how they will affect the current state.  The transitions specify how
each of the actions change the state.  Since an action could have
different effects, depending upon the state, we need to specify the
action's effect for each state in the <tt>MDP</tt> <p>

The most powerful aspect of the <tt>MDP</tt> is that the effects of an
action can be probabilistic.  Imagine we are specifying the effects of
doing action '<tt>a1</tt>' in state '<tt>s1</tt>'.  We could say that
the effect of '<tt>a1</tt>' is to leave the process in state
'<tt>s2</tt>', if there was no question about how '<tt>a1</tt>'
changes the world.  However, many decision processes have actions that
are not this simple.  Sometimes an action usually results in state
'<tt>s2</tt>', but occasionally it might result in state
'<tt>s3</tt>'.  <tt>MDP</tt>s allow you to specify these more complex
actions by allowing you to specify a set of resulting states and the
probability that each state results.

<h3>Immediate Rewards</h3>

If we want to automate the decision making process, then we must be
able to have some measure of an action's value so that we can compare
different actions.  We specify the immediate value for performing each
action in each state.  

<h3>The solution to an MDP</h3>

The solution to an <tt>MDP</tt> is called a <em>policy</em> and it
simply specifies the best action to take for each of the states.  For
the purposes of this tutorial, we will only concern ourselves with the
problem of finding the best policy assuming we will have a limited
lifetime.  For instance, assume that each day that we wake up we must
make a decision about something.  We want to make our decision
assuming that we will only have to make decisions for a fixed number
of days.  This type of solution is called a <em>finite horizon</em>
solution and the number of days we will be making decisions for will
be referred to as the <em>horizon length</em>. <p>

Although the policy is what we are after, we will actually compute a
<em>value function</em>.  Without going into the details, suffice it
to say that we can easily derive the policy if we have the value
function.  Thus, we will focus on finding this value function.  A
value function is similar to a policy, except that instead of
specifying an action for each state, it specifies a numerical value
for each state.

<h3>Who's this Markov guy?</h3>

Who this guy is isn't important now, but here we try to explain what
his name means as far as <tt>MDP</tt>s are concerned.<p>

When we talked about the transitions of the model, we said that we
simply needed to specify the resulting next state for each starting
state and action.  This assumes that the next state depends only upon
the current state (and action).  There are situations where the
effects of an action might depend not only on the current state, but
upon the last few states.  The <tt>MDP</tt> model will not let you
model these situations directly.  The assumption made by the
<tt>MDP</tt> model is that the next state is solely determined by the
current state (and current action).  This is called the
<em>Markov</em> assumption. <p>

We would say that the dynamics of the process are Markovian and this
has important ramifications for solving the problems.  The most
important is that our policies or value functions can have a simple
form.  The result is that we can choose our action based solely upon
the current state.  <p>

<P>
<center><h2><a href="./mdp-vi.html">Continue</a></h2></center>

<P>
<CENTER><IMG SRC="images/oak-a-stripe.gif">
</CENTER>

<P>
<CENTER><FONT SIZE="-1"><B>
<A HREF="index.html">POMDP Tutorial</A>
| <A HREF="mdp-vi.html">Next</A>
</B></FONT></CENTER>

</BODY>
</HTML>
